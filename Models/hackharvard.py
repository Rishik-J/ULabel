# -*- coding: utf-8 -*-
"""HackHarvard.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EXE4uj25528DNs-a8agULxqe1kZWhyqk
"""

!pip install spotipy

import spotipy
from spotipy.oauth2 import SpotifyClientCredentials

import numpy as np
np.random.seed(42)

import pandas as pd
pd.set_option('display.max_columns', None)

import ast

sp = spotipy.Spotify(client_credentials_manager=SpotifyClientCredentials(
    client_id='4242fd58f1d04c198cad287715296466',
    client_secret='a33e2be67e884f838d6debdb6e5ffb48'))
sp.category('hiphop', country='US')

playlist_link = "https://open.spotify.com/playlist/37i9dQZEVXbNG2KDcFcKOF?si=1333723a6eff4b7f"
playlist_URI = playlist_link.split("/")[-1].split("?")[0]
track_uris = [x["track"]["uri"] for x in sp.playlist_tracks(playlist_URI)["items"]]
len(track_uris)

playlist_link = "https://open.spotify.com/playlist/37i9dQZF1DWUZv12GM5cFk"
playlist_URI2 = playlist_link.split("/")[-1].split("?")[0]
track_uris2 = [x["track"]["uri"] for x in sp.playlist_tracks(playlist_URI)["items"]]

track_uris.extend(track_uris2)
len(track_uris)

['Track Name', 'Genre', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms']



df = pd.DataFrame(columns=['Track Name', 'Popularity', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms'])

for track in sp.playlist_tracks(playlist_URI)["items"]:
    #URI
    track_uri = track["track"]["uri"]
    
    #Track name
    track_name = track["track"]["name"]
    
    #Main Artist
    artist_uri = track["track"]["artists"][0]["uri"]
    artist_info = sp.artist(artist_uri)
    
    #Name, popularity, genre
    artist_name = track["track"]["artists"][0]["name"]
    artist_pop = artist_info["popularity"]
    artist_genres = artist_info["genres"]
    
    #Album
    album = track["track"]["album"]["name"]
    
    #Popularity of the track
    track_pop = track["track"]["popularity"]

    fetures = ['danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms']

    # Audio Features
    x = list(sp.audio_features(track_uri))
    arr = []
    arr.append(track_name)
    arr.append(track_pop)
    for el in fetures:
      arr.append(x[0][el])
    df.loc[len(df.index)] = arr

for track in sp.playlist_tracks(playlist_URI2)["items"]:
    #URI
    track_uri = track["track"]["uri"]
    
    #Track name
    track_name = track["track"]["name"]
    
    #Main Artist
    artist_uri = track["track"]["artists"][0]["uri"]
    artist_info = sp.artist(artist_uri)
    
    #Name, popularity, genre
    artist_name = track["track"]["artists"][0]["name"]
    artist_pop = artist_info["popularity"]
    artist_genres = artist_info["genres"]
    
    #Album
    album = track["track"]["album"]["name"]
    
    #Popularity of the track
    track_pop = track["track"]["popularity"]

    fetures = ['danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms']

    # Audio Features
    x = list(sp.audio_features(track_uri))
    arr = []
    arr.append(track_name)
    arr.append(track_pop)
    for el in fetures:
      arr.append(x[0][el])
    df.loc[len(df.index)] = arr

df= df.drop_duplicates(keep='first')
df.head()

# Building Neural Network
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense
from sklearn.model_selection import train_test_split

dfnn = df.drop(['Track Name'], axis=1)
train, test = train_test_split(dfnn, test_size=0.2, train_size=0.8, random_state=None, shuffle=True, stratify=None)

print(test.shape)
print(train.shape)

x_train = train.drop(['Popularity'], axis=1)
y_train = train['Popularity']
x_test = test.drop(['Popularity'], axis=1)
y_test = test['Popularity']

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler().fit(train)
scaler2 = StandardScaler().fit(test)
x_train = scaler.transform(train)
y_train = scaler.transform(train)
x_test = scaler2.transform(test)
y_test = scaler2.transform(test)

print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

model = Sequential()
model.add(Dense(100, activation='elu'))
model.add(Dense(80, activation='relu'))
model.add(Dense(60, activation='gelu'))
model.add(Dense(40, activation='relu'))
model.add(Dense(20, activation='gelu'))
model.add(Dense(24, activation='relu'))
model.add(Dense(20, activation='gelu'))
model.add(Dense(13, activation='sigmoid'))
model.compile(loss='MeanAbsoluteError',
optimizer='adamax',
metrics=['MeanSquaredError'])
model.fit(x_train, y_train, epochs=80, batch_size=1, verbose=1)

normalized_df=(dfnn-dfnn.mean())/dfnn.std()

type(normalized_df)

# copy the data
df_max_scaled = dfnn.copy()
  
# apply normalization techniques
for column in df_max_scaled.columns:
    df_max_scaled[column] = df_max_scaled[column]  / df_max_scaled[column].abs().max()

df_max_scaled

df_max_scaled.to_excel("output.xlsx")

train, test = train_test_split(df_max_scaled, test_size=0.2, train_size=0.8, random_state=None, shuffle=True, stratify=None)

x_train = train.drop(['Popularity'], axis=1)
y_train = train['Popularity']
x_test = test.drop(['Popularity'], axis=1)
y_test = test['Popularity']

model = Sequential()
model.add(Dense(100, activation='elu'))
model.add(Dense(80, activation='relu'))
model.add(Dense(60, activation='gelu'))
model.add(Dense(40, activation='relu'))
model.add(Dense(20, activation='gelu'))
model.add(Dense(24, activation='relu'))
model.add(Dense(20, activation='gelu'))
model.add(Dense(13, activation='sigmoid'))
model.compile(loss='MeanAbsoluteError',
optimizer='adamax',
metrics=['MeanSquaredError'])
model.fit(x_train, y_train, epochs=80, batch_size=1, verbose=1)

